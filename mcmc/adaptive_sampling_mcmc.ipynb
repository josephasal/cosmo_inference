{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hx8xbtqF7NYb41E32ANAwqp4mfaywCRM",
      "authorship_tag": "ABX9TyNxMMP/bZvj9t+SFnX+yDZg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephasal/cosmo_inference/blob/diagnostics-%2B-adaptive-sampling/mcmc/adaptive_sampling_mcmc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Contains the core code for the MCMC algorithm implementated\n",
        "#Have now made this one use adaptive samplling, adjusting the proposal as it goes along\n",
        "\n",
        "#Dimensionless distance modulus function implementation\n",
        "import numpy as np\n",
        "#Distance modulus function\n",
        "def calculate_distance_modulus(z, omega_m,h):\n",
        "  \"\"\"\n",
        "  Calculates dimensionless theoretical distance modulus using\n",
        "\n",
        "  inputs:\n",
        "   - z: Redshift\n",
        "   - omega_m: density matter parameter\n",
        "   - h: dimensionless hubble constance H0 = 100h km/s/Mpc\n",
        "\n",
        "   outputs: theoretical distance modulus\n",
        "  \"\"\"\n",
        "  c = 299792.458   # speed of light in km/s\n",
        "  H0 = 100 * h     # Hubble constant in km/s/Mpc\n",
        "\n",
        "  #Luminosity distance based on Penn 1999 analytic solution\n",
        "\n",
        "  #Fitting function\n",
        "  def eta(a,omega_m):\n",
        "    \"\"\"\n",
        "    Fits eta\n",
        "    inputs:\n",
        "      a - a number\n",
        "      omega_m - matter density\n",
        "\n",
        "    outputs: eta as a function of a and omega_m\n",
        "    \"\"\"\n",
        "    s = ((1-omega_m)/omega_m)**(1/3)\n",
        "    eta = 2*np.sqrt(s**3 +1) * ((1/(a**4)) - 0.1540*(s/(a**3)) + 0.4304 *((s**2)/(a**2)) + 0.19097*((s**3)/a) + 0.066941*(s**4))**(-1/8)\n",
        "\n",
        "    return eta\n",
        "\n",
        "  #Calculate eta for 1 and 1/z+1\n",
        "  a = 1/(z+1)\n",
        "  eta_1 = eta(1,omega_m)\n",
        "  eta_z = np.array([eta(ai, omega_m) for ai in a])\n",
        "\n",
        "  #Dimensionless luminosity distance calculation\n",
        "  d_L_star = (c/H0) * (1+z) * (eta_1 - eta_z)\n",
        "\n",
        "\n",
        "\n",
        "  #Now to calculate distance modulus mu\n",
        "  theoretical_mu = 25 - 5*np.log10(h) + 5*np.log10(d_L_star)\n",
        "  return theoretical_mu"
      ],
      "metadata": {
        "id": "a7aN07BDYdE0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MCMC basic implementation\n",
        "\n",
        "#Likelihood function, standard gaussian function\n",
        "\n",
        "def log_likelihood(mu_obs, mu_model, sigma_mu):\n",
        "\n",
        "  \"\"\"\n",
        "  Computes thes logged likelihood for the sample and observed distance modulus\n",
        "\n",
        "  inputs:\n",
        "    - mu_obs: observed mu (mu from the data)\n",
        "    - mu_model: theoretical mu from the model\n",
        "    - sigma_mu: standard deviation of the observed mu (uncertainty)\n",
        "\n",
        "  outputs:\n",
        "    - log likelihood\n",
        "\n",
        "  \"\"\"\n",
        "  return -0.5 * np.sum((mu_obs - mu_model)**2/sigma_mu**2)\n",
        "\n",
        "\n",
        "# Defining the prior as a function\n",
        "def log_prior(params):\n",
        "  \"\"\"\n",
        "  Function that sets a uniform prior of omega m and h\n",
        "\n",
        "  \"\"\"\n",
        "  omega_m, h = params\n",
        "  if 0.1 < omega_m < 0.5 and 0.4 < h < 0.9:\n",
        "    return 0.0\n",
        "\n",
        "  else:\n",
        "    return -np.inf #acceptance probability is 0 if not in the priors upper and lower bounds\n",
        "\n",
        "\n",
        "\n",
        "#Metropolis Hastings algorithm\n",
        "\n",
        "def adaptive_metropolis_hastings(likelihood, z, mu_obs, sigma_mu, n_steps, initial_params, step_size, burn_in, n_walkers):\n",
        "  \"\"\"\n",
        "  Perform Metropolis Hastings MCMC to sample from the posterior\n",
        "\n",
        "  inputs:\n",
        "    - likelihood: function to compute the likelihood\n",
        "    - z: redshift\n",
        "    - mu_obs: observed mu\n",
        "    - sigma_mu: standard deviation of the observed mu\n",
        "    - n_steps: Number of steps for MCMC\n",
        "    - intial_params: initial guesses for [omega_m, h] for each walker, has to be array with rows = number of walkers\n",
        "    - step size: proposal step size for [omega_m, h]\n",
        "    - burn_in : percentage of chain to discard for the burn in period (given as a decimal)\n",
        "    - n_walkers: number of walkers that are sampling\n",
        "\n",
        "  outputs:\n",
        "  Array of 3 dimensions, in shape of (steps after burn in , walker number, 2)\n",
        "\n",
        "  \"\"\"\n",
        "  params = np.array(initial_params) #input as [] bracket so just make an array\n",
        "  samples = []\n",
        "  accepted_samples = np.zeros(n_walkers) #gonna use to calculate acceptance rate\n",
        "\n",
        "  #New adaptive parameters:\n",
        "  update_interval = 100  #update every 100 iterations\n",
        "  target_alpha = 0.25 #set a target acceptance rate of 25%\n",
        "  learning_rate = 0.1 #adaptation speed parameter\n",
        "  accepted_cycle = np.zeros(n_walkers)  #empty array of accepted proposals at each cycle/iteration\n",
        "\n",
        "\n",
        "  for step in range (n_steps):\n",
        "    new_params = np.empty_like(params) #empty array same size as intial parameters array, need to keep track of new paramters for each walker via matrix\n",
        "\n",
        "    #Do similar calulation for new parameters as before but just have to loop it for all walkers now\n",
        "    for i in range(n_walkers):\n",
        "\n",
        "      #Proposal\n",
        "      #guess for new parameters, propsal distribution is a multivariate gaussian distribution now\n",
        "      #Step size has to be a 2x1 array\n",
        "      sigma_omega_m = step_size[0]\n",
        "      sigma_h = step_size[1]\n",
        "\n",
        "      #covariance matrix thing off diagonals are the correlation between the values\n",
        "      rho = 0 #set as 0 intially, alogrithm will see if there are correlations --> explores parameter space better\n",
        "      covariance_matrix = np.array([[sigma_omega_m**2, rho * sigma_omega_m * sigma_h], [rho * sigma_omega_m * sigma_h, sigma_h**2]])\n",
        "\n",
        "      #drawing from this new proposal now but for each walker/ walker i\n",
        "      proposed_params = params[i] + np.random.multivariate_normal(np.zeros(2), covariance_matrix)\n",
        "      omega_m_proposed, h_proposed = proposed_params\n",
        "\n",
        "      # Priors on Omega_m and h, using our new functions\n",
        "      log_prior_proposed = log_prior(proposed_params)\n",
        "\n",
        "      #If prior is  - infininty then proposal out of bounds so reject\n",
        "      if np.isneginf(log_prior_proposed):\n",
        "        new_params[i] = params[i]\n",
        "        continue\n",
        "\n",
        "      #Distance modulus and log likelihood of proposed parameters\n",
        "      proposed_mu_model = calculate_distance_modulus(z, proposed_params[0], proposed_params[1]) #now for each walker\n",
        "      proposed_log_likelihood = log_likelihood(mu_obs, proposed_mu_model, sigma_mu)\n",
        "\n",
        "\n",
        "      #Posterior for proposed parameters\n",
        "      proposed_log_posterior = proposed_log_likelihood + log_prior_proposed\n",
        "\n",
        "      #Distance modulus, log likelihood and posterior of current parameters, initially inputted from the function\n",
        "      current_mu_model = calculate_distance_modulus(z, params[i,0], params[i,1])  #now for each walker\n",
        "      current_log_likelihood = log_likelihood(mu_obs, current_mu_model, sigma_mu)\n",
        "      current_log_posterior = current_log_likelihood + log_prior(params[i])\n",
        "\n",
        "      #Calculate the acceptance probability, now based on log posteriors\n",
        "      delta_log_posterior = proposed_log_posterior - current_log_posterior\n",
        "\n",
        "      #Implementing explicit overflow protection\n",
        "      #If the difference in likelihoods is at max python limit, then accept the new proposal with probaiblity 1. Means new parameters are leng\n",
        "      if delta_log_posterior > 700:\n",
        "        acceptance_probability = 1.0\n",
        "\n",
        "      #If difference in likelihood is at min python limit, then dont accept the new proposal at all. Means new parameters are clapped\n",
        "      elif delta_log_posterior < -700:\n",
        "        acceptance_probability = 0.0\n",
        "\n",
        "      #If difference something else then we accept with probability below and randomly sample. Lets us explore parameter space\n",
        "      else:\n",
        "        acceptance_probability = min(1, np.exp(delta_log_posterior))\n",
        "\n",
        "\n",
        "      u = np.random.uniform(0,1) #set the randomness part of accept/ reject\n",
        "\n",
        "      #Accept proposed move\n",
        "      if u < acceptance_probability:\n",
        "        new_params[i] = proposed_params\n",
        "        accepted_samples[i] += 1\n",
        "        accepted_cycle[i] += 1 #also add acceptance count for each walker adaptation thing\n",
        "      else:\n",
        "        new_params[i] = params[i] #chain doesnt move and retrys sampling\n",
        "\n",
        "    #Updating all the walkers at the same time, outside of the loop, do end of every loop\n",
        "    params = new_params.copy()\n",
        "    samples.append(params.copy())\n",
        "\n",
        "    #Adaptive update after every update interval iteration loop\n",
        "    if (step +1) % update_interval == 0: #check if nth+1 iteration is exact multiple, so saying it has don 100/200/300 steps\n",
        "      average_acceptance = np.mean(accepted_cycle)/ update_interval #acceptance rate of this cycle (100 steps)\n",
        "\n",
        "      #Increase step size of alpha larger than target, decrease step size if alpha is too small\n",
        "      #Robbins Munro approximation/ Haario et al.\n",
        "      scale_factor = np.exp(learning_rate * (average_acceptance - target_alpha))\n",
        "\n",
        "      #Now scale both step sizes for omega_m and h by this scale factor\n",
        "      step_size = [step_size[0]* scale_factor, step_size[1]* scale_factor] #list comprehension innit\n",
        "\n",
        "      #Something to tell me what is happening\n",
        "      print(f\"After {step+1} iteration alpha = {average_acceptance}, new step size = {step_size}\")\n",
        "\n",
        "      #Reset counter\n",
        "      accepted_cycle = np.zeros(n_walkers)\n",
        "\n",
        "  #Acceptance ratio calculation\n",
        "  acceptance_ratio = accepted_samples/ n_steps\n",
        "  print(f\"MCMC carried out with {n_steps} steps, and acceptance ratio of each walker {acceptance_ratio}\")\n",
        "\n",
        "\n",
        "  #Number of samples to discard from the chain due to burn in\n",
        "  burned_chains = int(burn_in * n_steps) #keep integer\n",
        "  samples_post_burn = samples[burned_chains:] #use everything after the burn in number\n",
        "\n",
        "  return np.array(samples_post_burn)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ulH5kLteoxwo"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}